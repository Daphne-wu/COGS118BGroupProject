{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# COGS 118B - Final Project"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Names\n",
    "\n",
    "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
    "\n",
    "- Audrey Liang\n",
    "- Geena Limfat\n",
    "- Nate Mead\n",
    "- Neha Sharma\n",
    "- Daphne Wu"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents \n",
    "- the solution/what you did\n",
    "- major results you came up with (mention how results are measured) \n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Background\n",
    "\n",
    "As technology and social media becomes more integrated into our society, it is becoming an increasingly viable tool for tracking social trends on larger scales than previously available. One of the most heartbreaking and devastating social trends in any society is suicide prevelance. Understanding correlations between real-world suicide rates and online discussions of mental health issues could give invaluable insights into predicting and surveilling related trends and potential risk factors. There has been a great deal of research monitoring suicide rates in the United States and abroad. Other bodies of research have also been dedicated to tracking social media as a measurement tool for the prevalence of mental health issues. One such study assessed linguistic trends shared by Twitter users diagnosed with depression, using a probabilistic model to determine if certain posting habits could indicate major depression in individuals <a name=\"DeChoudhury\"></a>[<sup>[1]</sup>](#DeChoudhury). Published in 2013, it indicated that 27 million Americans likely suffer from clinical depression, which was predicted to account for more than 30,000 suicides per year at the time. Since then, the population of the United States, prevalence of social media in everyday life, and suicide rates within the nation have all increased. According to the National Institute of Mental Health, surveys in 2020 tracked reported suicides within the United States as having reached past 45,000 <a name=\"nimhnote\"></a>[<sup>[3]</sup>](#NIMH). This is a 50% increase in a matter of 7 years. \n",
    "\n",
    "Additionally, there are studies that showed a positive correlation between cyberbullying, depression, suicidal ideation, suicidal plans, and suicide attempts, along with correlation between internet addiction and suicidality. Cyberbullying not only affects the victim, but also the perpertrator(s) as well which exposes at least 2 individuals to higher depression and behavioral issues. Cyberbullying also poses a greater threat as studies have shown that bullying through the internet increases the risk of suicidal ideation by a factor of 3.12 compared to traditional bullying’s factor of 2.16. <a name=\"ncbinote\"></a>[<sup>[4]</sup>](#ncbi). The more negative feedback that children receive onlines from strangers lowers their self-esteem and feelings of self-worth, and the more time spend on the internet increases the amount of exposure to these comments. Online victims are also more isolated than in person victims, making it more difficult for others to witness and potentially intervene. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem Statement\n",
    "\n",
    "Our project aims to identify tweets indicating suicide risk through sentiment analysis. After preprocessing the data using term frequency-inverse document frequency (tf-idf), VADER sentiment analysis, and principal component analysis (PCA) for dimensionality reduction, we further analyzed the data using clustering techniques such as K-means and DBSCAN. In particular, we utilized the VADER sentiment analysis tool in conjunction with a bag-of-words approach, which involves categorizing words as positive or negative to assess the sentiment of tweets. We will employ a pre-trained sentiment analysis model like VADER, which can be directly imported and applied to the tweet data to extract sentiment scores. For sentiment analysis, we are utilizing a bag-of-words approach with predefined lists of positive and negative words, omitting the need for word embeddings. \n",
    "\n",
    "Our problem is quantifiable as it involves analyzing sentiment scores assigned to tweets, which can be expressed mathematically. The problem is measurable through metrics such as sentiment polarity scores and clustering performance metrics like silhouette score or Davies–Bouldin index. Additionally, the problem is replicable as it can be reproduced with different datasets and contexts, ensuring the generalizability of findings.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data\n",
    "\n",
    "We obtained the tweet data from the Twitter API using the Tweepy Python library. The dataset consists of approximately 10,000 tweets related to various topics, including those indicating suicide risk. Before analysis, we performed data cleaning to remove duplicates, irrelevant tweets, and retweets.\n",
    "\n",
    "### Dataset Source: Twitter API\n",
    "- Dataset Size: The dataset comprises approximately 10,000 tweets.\n",
    "- Observation Description: Each observation represents a single tweet retrieved from the Twitter API. An observation consists of several variables, including the tweet text, user ID, timestamp, and metadata such as retweet count and favorite count.\n",
    "- Critical Variables:\n",
    "    - Tweet Text: Contains the actual content of the tweet.\n",
    "    - Suicide Classification: Whether or not it is a suicidal post. \n",
    "\n",
    "### Data Cleaning Process:\n",
    "Removing Empty Rows: We dropped any empty rows present in the dataset.\n",
    "Punctuation Removal: We removed punctuation from the tweet text to ensure consistency in text processing.\n",
    "\n",
    "The detailed code for data cleaning can be found in this ADD LINK BROOOOOO!!!!! notebook, while the summary of the cleaning process is provided here for readability.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Proposed Solution\n",
    "\n",
    "The approach we are using to dentify tweets that indicate suicide risk involves a multiple step process to leverage sentiment analysis techniques combined with clustering algorithms. After preprocessing the tweets using TF-IDF (Term Frequency-Inverse Document Frequency) to extract features, we apply the VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analysis tool to assign sentiment scores to each tweet. Then, we reduce the dimensionality of the feature space using Principal Component Analysis (PCA) to capture the most significant information while minimizing computational complexity. After the preprocessing steps, we will use two clustering algorithms, K-means and DBSCAN, to partition the tweets into distinct groups based on their sentiment and content similarities. K-means partitions the data into K clusters, where K is determined through experimentation or domain knowledge. DBSCAN, on the other hand, identifies dense regions of tweets in the feature space, allowing for the detection of outliers and noise. To incorporate sentiment analysis with a bag-of-words approach using VADER, we assign sentiment scores to individual words in each tweet based on their presence in lists of positive and negative words. These sentiment scores are then aggregated to derive an overall sentiment score for each tweet.\n",
    "\n",
    "### Implementation:\n",
    "\n",
    "- Preprocessing: We'll use Python's scikit-learn library to perform TF-IDF transformation and PCA for dimensionality reduction.\n",
    "- Sentiment Analysis: VADER sentiment analysis will be implemented using the nltk library in Python.\n",
    "- Clustering Algorithms: K-means and DBSCAN will be implemented using the scikit-learn library.\n",
    "- Bag-of-Words Approach: We'll use custom scripts to assign sentiment scores to words based on predefined lists of positive and negative words.\n",
    "\n",
    "### Evaluation:\n",
    "To evaluate the effectiveness of our solution, we'll use labeled datasets to assess the accuracy of identifying tweets indicating suicide risk. We'll compare the clustering results obtained from K-means and DBSCAN against the ground truth labels to measure clustering performance. Additionally, we'll assess the correlation between sentiment scores assigned to tweets and their corresponding ground truth labels.\n",
    "\n",
    "### Benchmark Model:\n",
    "As a benchmark model, we'll compare our solution's performance against a baseline model that employs basic text processing techniques without sentiment analysis. This baseline model may involve simple clustering algorithms applied directly to the TF-IDF transformed data without incorporating sentiment scores.\n",
    "\n",
    "With this approach, our goal is to to provide a robust and interpretable solution for identifying tweets indicating suicide risk, using te power of sentiment analysis and clustering techniques."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms).\n",
    "\n",
    "\n",
    "scores closer to 1 indicates better model performace, 1 being perfect presicion and recall for F1 or signifying the data calustering are in perfect agreement with the true labels:\n",
    "\n",
    "\n",
    "- **F1 Score**: 0.9311377245508982 - measure of model accuracy considering presicion and recall of the model's predictions. Particularly usefil in binary classifications (which we are doing). K-means produced the highest F1 score compared to GMM and DBSCAN. \n",
    "- **K-Means Rand**: 0.6050382484832498 - computes similarity between two clusters. K-means produced the second highest score of 0.605 this indicates moderate agreement between the k-means clusters and the truth labels. \n",
    "- **K-Means Adjusted Rand**: 0.1793769407924069 - the rand index but corrected-for-chance. As it is relatively low, there may be some but not a strong agreement between clustering labels and truth labels, and some agreement may be due to chance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Results:\n",
    "## Analysis of Tweet Data for Identifying Suicidal Tendencies\n",
    "\n",
    "The primary goal of our research is to detect and uncover patterns within tweet data that could suggest a risk of suicide. We do this using unsupervised machine learning techniques. We utilize the textual content of tweets, transform them into numerical data through Bag of Words (BoW) and sentiment analysis using VADER. Following this, we perform dimensionality reduction, clustering, and model evaluation techniques in order to analyze our data effectively.\n",
    "\n",
    "### Data Preprocessing and Feature Extraction\n",
    "We began by loading our dataset, which includes tweet texts and a binary indicator of suicidal tendency (whether or not the post might be suicidal). However, our analysis focused solely on the text and we dropped the column of data that would indicate suicidal tendency. After excluding missing data (dropping empty rows), we processed 1000 features using BoW and appended sentiment scores from VADER to our feature set.\n",
    "\n",
    "### Dimensionality Reduction with PCA\n",
    "Principal Component Analysis (PCA) was used to reduce the dimensionality of our feature set, simplifying the high-dimensional BoW data into two principal components for more ease of visualization and clustering.\n",
    "\n",
    "### Determining Optimal Clusters with Elbow Method\n",
    "We used the Elbow Method to identify the optimal number of clusters for K-Means clustering. Our analysis found a distinct \"elbow\" at \\( k=2 \\), which suggested that two clusters was the best choice for clustering our data (see Figure below).\n",
    "\n",
    "<img src=\"pics/kmeansElbow.png\" alt=\"Elbow Method\" width=\"400\" height=\"300\"/>\n",
    "\n",
    "\n",
    "### Clustering with K-Means\n",
    "We applied K-Means with the chosen number of clusters which revealed two distinct groupings in our PCA-reduced feature space, as shown in the figure below. The centroids of the clusters are marked with a red x that indicates the central points around which the tweets are grouped.\n",
    "\n",
    "<img src=\"pics/kmeansCluster.png\" alt=\"K-Means Clustering with PCA\" width=\"400\" height=\"300\"/>\n",
    "\n",
    "#### Evaluation metrics for K-means clustering: \n",
    "- **F1 Score**: 0.9311377245508982 - K-means produced the highest F1 score compared to GMM and DBSCAN. \n",
    "- **K-Means Rand**: 0.6050382484832498 - K-means produced the second highest score of 0.605 this indicates moderate agreement between the k-means clusters and the truth labels. \n",
    "- **K-Means Adjusted Rand**: 0.1793769407924069 - K-means produces a relatively low score. There may be some but not a strong agreement between clustering labels and truth labels, and some agreement may be due to chance.\n",
    " \n",
    "The main objective is to closely reflect the labeled classification of tweets into \"suicidal\" or \"not suicidal\". In this case, the high F1 Score suggests success, however, the low adjusted rand index cautions an overstated success. \n",
    "\n",
    "### Model Selection with Gaussian Mixture Model (GMM)\n",
    "To verify the clustering results and explore alternative models, we fitted a Gaussian Mixture Model (GMM). By using the Akaike Information Criterion (AIC), we identified the best fit model complexity. The AIC suggested a preference for a model with fewer components. This aligned with our K-Means results, as depicted in the figure below. The resulting clusters from the GMM, as visualized, and it also suggested a meaningful segmentation of the data.\n",
    "\n",
    "<img src=\"pics/AICGMM.png\" alt=\"AIC for Model Selection\" width=\"400\" height=\"300\"/>\n",
    "<img src=\"pics/GMMCluster.png\" alt=\"Gaussian Mixture Model Clustering\" width=\"400\" height=\"300\"/>\n",
    "\n",
    "#### Evaluation metrics for GMM: \n",
    "- **GMM F1 Score**: 0.9304973037747154 - produced the second highest score behind k-means, sugessting that the clusters produced by GMM align with the true binary lables in the tweet dataset. \n",
    "- **GMM Rand**: 0.6040120083908003 - produced the lowest score of 0.604. Still, it indicates moderate agreement between the GMM clusters and the truth labels.\n",
    "- **GMM Adjusted Rand**: 0.177437612438209 - produced a relatively low score. There may be some but not a strong agreement between clustering labels and truth labels, though some agreement may still be due to chance.\n",
    "\n",
    "As mentioned, the main objective is to closely reflect the labeled classification of tweets into \"suicidal\" or \"not suicidal\". Similar to the K-means clustering, GMM clustering produces a high F1 Score suggesting success, however, the low adjusted rand index indicated that there may be an overstated success. \n",
    "\n",
    "### Alternative Clustering with DBSCAN\n",
    "We also took a look at using Density-Based Spatial Clustering of Applications with Noise (DBSCAN) as an alternative method to identify clusters based on density. However, the results we got, shown in the figure below, did not yield distinct clusters when compared to K-Means or GMM.\n",
    "\n",
    "<img src=\"pics/DBSCANCluster.png\" alt=\"DBSCAN Clustering with PCA\" width=\"400\" height=\"300\"/>\n",
    "\n",
    "#### Evaluation metrics for DBSCAN: \n",
    "- **DBSCAN F1 Score**: 0.908590645062672 - produced a high score, but the lowest of the three models used. Still, it suggests that the clusters produced by DBSCAN align with the true binary lables in the tweet dataset. \n",
    "- **DBSCAN Rand**: 0.632388740249463 - produced the highest score between the models used of 0.604. It indicates more but still moderate agreement between the DBSCAN clusters and the truth labels.\n",
    "- **DBSCAN Adjusted Rand**: 0.2432863426143979 - produced the highest score of the three models. Compared to the other models, there more of an agreement between clustering labels and truth labels, and some agreement may be due to chance.\n",
    "\n",
    "### Discussion and Interpretation of Results\n",
    "The results of the K-Means and GMM clustering, supported by the Elbow Method and AIC metrics to better ensure the best results, suggested that the tweet dataset comprised of two principal groupings. The two groupings may be correlated with tweets of different sentiment valences that are potentially related to the risk of suicide.\n",
    "\n",
    "- **Main Point**: Our PCA combined with K-Means clustering indicates two distinct groups of tweets that could represent whether or not the tweet was a suicide post.\n",
    "  \n",
    "- **Secondary Points**:\n",
    "  - The Elbow Method provided a clear indication for the number of clusters, which was substantiated by the AIC in GMM.\n",
    "  - DBSCAN did not perform as well as K-Means or GMM, highlighting the importance of choosing the right clustering algorithm based on the data characteristics.\n",
    "  - The F1 scores, Rand Index, and Adjusted Rand Index offered additional perspectives on the clustering quality, confirming the appropriateness of our model choices."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the Result\n",
    "\n",
    "**Main Point**: \n",
    "Our analysis indicated that machine learning can aid in providing valuable insights into identifying patterns associated with suicidal tendencies in social media texts. The high F1 Score attained through K-Means clustering suggests our model is effective in identifying and recognizing tweets of different emotional valences.\n",
    "\n",
    "**Secondary Points**:\n",
    "1. The moderate Rand Index score reflects a reasonable but imperfect alignment between the clusters formed and the pre-labeled data. This implies that while the clusters have some correlation with the labeled data, there is complexity in the data that the model may not be fully capturing.\n",
    "   \n",
    "2. The Adjusted Rand Index, being closer to 0 than to 1, suggests that the true label agreement could be partly coincidental. It indicates a need for a more nuanced approach or more informative features to capture the underlying structure of the data better.\n",
    "\n",
    "3. The exploration of clustering algorithms such as DBSCAN and GMM, alongside K-Means, demonstrates our comprehensive approach to understanding the dataset. Each model provided unique perspectives, with GMM aligning closely with K-Means in performance.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "While the results we got are promising, there are still limitations to the approach we used. The range of the data and the clustering methods we used are heavily reliant on the chosen hyperparameters. The exploration of a broader hyperparameter space may enhance model performance. Additionally, the dimensionality reduction through PCA, although may be beneficial for visualization, it could result in the loss of important information that may improve clustering accuracy. More data could also strengthen the model's robustness and allow for a more thorough understanding of the nuanced language patterns associated with suicide risks in text.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "There are inherent ethical considerations in using machine learning to analyze sensitive topics such as mental health. Ensuring the privacy of individuals' data is cruicial, and any deployment of such models in real-world applications must adhere to strict data protection standards. Unintended consequences, such as the misuse of predictive results or the potential stigmatization of individuals based on model output, must be proactively addressed. Tools like Deon provide ethical checklists that would be instrumental in guiding the development and deployment of models responsibly.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Our study's main takeaway is the potential of machine learning in identifying tweets with suicidal content, with the F1 Score substantiating the effectiveness of our clustering approach. The work fits within the broader context of mental health monitoring through social media, a growing field that leverages data analytics to offer timely interventions. Future work should look into richer feature sets, alternative dimensionality reduction techniques, and the ethical deployment of such models. Enhanced model tuning and validation, alongside collaboration with mental health professionals, could see these techniques contributing significantly to preventive healthcare measures."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('COGS118B_WI24')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d32b38566dea232296ad015676f5eb17bd884ce0b9315fa59364119c2a8275d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}