{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # import pandas as pd\n",
    "# # # from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# # # from sklearn.decomposition import TruncatedSVD\n",
    "# # # from sklearn.cluster import KMeans\n",
    "# # # from sklearn.model_selection import train_test_split\n",
    "# # # from sklearn.metrics import accuracy_score\n",
    "# # # from sklearn.pipeline import Pipeline\n",
    "# # # from nltk.corpus import stopwords\n",
    "# # # from nltk.stem import PorterStemmer\n",
    "# # # from sklearn.preprocessing import LabelEncoder\n",
    "# # # from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # # # Load data\n",
    "# # # data = pd.read_csv('suicide.csv')\n",
    "\n",
    "# # # # Preprocessing\n",
    "# # # stop_words = set(stopwords.words('english'))\n",
    "# # # stemmer = PorterStemmer()\n",
    "\n",
    "# # # def preprocess_text(text):\n",
    "# # #     text = text.lower()\n",
    "# # #     text = ' '.join([word for word in text.split() if word.isalpha() and word not in stop_words])\n",
    "# # #     text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "# # #     return text\n",
    "\n",
    "# # # data['clean_text'] = data['Tweet'].apply(preprocess_text)\n",
    "\n",
    "# # # # Feature extraction\n",
    "# # # tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "# # # tfidf_matrix = tfidf_vectorizer.fit_transform(data['clean_text'])\n",
    "\n",
    "# # # # Dimensionality reduction\n",
    "# # # svd = TruncatedSVD(n_components=100)\n",
    "# # # svd_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# # # # Clustering\n",
    "# # # kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# # # kmeans.fit(svd_matrix)\n",
    "\n",
    "# # # # Assign cluster labels\n",
    "# # # data['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# # # # Split data into train and test sets\n",
    "# # # X_train, X_test, y_train, y_test = train_test_split(svd_matrix, data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# # # # Train classifier\n",
    "# # # classifier = LogisticRegression()\n",
    "# # # classifier.fit(X_train, y_train)\n",
    "\n",
    "# # # # Predictions\n",
    "# # # y_pred = classifier.predict(X_test)\n",
    "\n",
    "# # # # Evaluate classifier\n",
    "# # # accuracy = accuracy_score(y_test, y_pred)\n",
    "# # # print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# # import pandas as pd\n",
    "# # from sklearn.feature_extraction.text import CountVectorizer\n",
    "# # from sklearn.decomposition import TruncatedSVD\n",
    "# # from sklearn.cluster import KMeans\n",
    "# # from sklearn.model_selection import train_test_split\n",
    "# # from sklearn.metrics import accuracy_score\n",
    "# # from sklearn.pipeline import Pipeline\n",
    "# # from nltk.corpus import stopwords\n",
    "# # from nltk.stem import PorterStemmer\n",
    "# # from sklearn.preprocessing import LabelEncoder\n",
    "# # from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # # Load data\n",
    "# # data = pd.read_csv('suicide.csv')\n",
    "\n",
    "# # # Preprocessing\n",
    "# # stop_words = set(stopwords.words('english'))\n",
    "# # stemmer = PorterStemmer()\n",
    "\n",
    "# # def preprocess_text(text):\n",
    "# #     text = text.lower()\n",
    "# #     text = ' '.join([word for word in text.split() if word.isalpha() and word not in stop_words])\n",
    "# #     text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "# #     return text\n",
    "\n",
    "# # data['clean_text'] = data['Tweet'].apply(preprocess_text)\n",
    "\n",
    "# # # Feature extraction (Bag of Words)\n",
    "# # count_vectorizer = CountVectorizer(max_features=1000)\n",
    "# # bow_matrix = count_vectorizer.fit_transform(data['clean_text'])\n",
    "\n",
    "# # # Dimensionality reduction\n",
    "# # svd = TruncatedSVD(n_components=100)\n",
    "# # svd_matrix = svd.fit_transform(bow_matrix)\n",
    "\n",
    "# # # Clustering\n",
    "# # kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# # kmeans.fit(svd_matrix)\n",
    "\n",
    "# # # Assign cluster labels\n",
    "# # data['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# # # Split data into train and test sets\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(svd_matrix, data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# # # Train classifier\n",
    "# # classifier = LogisticRegression()\n",
    "# # classifier.fit(X_train, y_train)\n",
    "\n",
    "# # # Predictions\n",
    "# # y_pred = classifier.predict(X_test)\n",
    "\n",
    "# # # Evaluate classifier\n",
    "# # accuracy = accuracy_score(y_test, y_pred)\n",
    "# # print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# # Load data\n",
    "# data = pd.read_csv('suicide.csv')\n",
    "\n",
    "# # Drop rows with missing values in the 'Tweet' column\n",
    "# data.dropna(subset=['Tweet'], inplace=True)\n",
    "\n",
    "# # Feature extraction (Bag of Words)\n",
    "# count_vectorizer = CountVectorizer(max_features=1000)\n",
    "# bow_matrix = count_vectorizer.fit_transform(data['Tweet'])\n",
    "\n",
    "# # Dimensionality reduction\n",
    "# svd = TruncatedSVD(n_components=100)\n",
    "# svd_matrix = svd.fit_transform(bow_matrix)\n",
    "\n",
    "# # Clustering\n",
    "# kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# kmeans.fit(svd_matrix)\n",
    "\n",
    "# # Assign cluster labels\n",
    "# data['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# # Evaluate clustering results (compare with Suicide labels)\n",
    "# # Implement your evaluation steps here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# # Load data\n",
    "# data = pd.read_csv('suicide.csv')\n",
    "\n",
    "# # Drop rows with missing values in the 'Tweet' column\n",
    "# data.dropna(subset=['Tweet'], inplace=True)\n",
    "\n",
    "# # Feature extraction (Bag of Words)\n",
    "# count_vectorizer = CountVectorizer(max_features=1000)\n",
    "# bow_matrix = count_vectorizer.fit_transform(data['Tweet'])\n",
    "\n",
    "# # Dimensionality reduction with PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_matrix = pca.fit_transform(bow_matrix.toarray())\n",
    "\n",
    "# # Clustering\n",
    "# kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# kmeans.fit(pca_matrix)\n",
    "\n",
    "\n",
    "# # Assign cluster labels\n",
    "# data['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# # Plotting the clusters\n",
    "# plt.figure(figsize=(8, 6))\n",
    "\n",
    "# # Plot each cluster separately\n",
    "# for cluster_label in range(2):\n",
    "#     plt.scatter(pca_matrix[data['cluster_label'] == cluster_label, 0], \n",
    "#                 pca_matrix[data['cluster_label'] == cluster_label, 1], \n",
    "#                 label=f'Cluster {cluster_label}', alpha=0.5)\n",
    "\n",
    "# # Plot centroids\n",
    "# centroids = kmeans.cluster_centers_\n",
    "# plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=100, c='black', label='Centroids')\n",
    "# # \n",
    "# plt.title('K-Means Clustering of Tweets')\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Visualize clustering results\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(pca_matrix[:, 0], pca_matrix[:, 1], c=data['cluster_label'], cmap='viridis', alpha=0.5)\n",
    "# plt.title('PCA Clustering of Tweets')\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.colorbar(label='Cluster Label')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.cluster import KMeans\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# # Load data\n",
    "# data = pd.read_csv('suicide.csv')\n",
    "\n",
    "# # Drop rows with missing values in the 'Tweet' column\n",
    "# data.dropna(subset=['Tweet'], inplace=True)\n",
    "\n",
    "# # Initialize VADER sentiment analyzer\n",
    "# sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# # Calculate sentiment scores for each tweet\n",
    "# data['compound_score'] = data['Tweet'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "\n",
    "# # Feature extraction (Bag of Words)\n",
    "# count_vectorizer = CountVectorizer()\n",
    "# bow_matrix = count_vectorizer.fit_transform(data['Tweet'])\n",
    "\n",
    "# # Dimensionality reduction with PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_matrix = pca.fit_transform(bow_matrix.toarray())\n",
    "\n",
    "# # Clustering with K-means\n",
    "# kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# kmeans.fit(pca_matrix)\n",
    "\n",
    "# # Assign cluster labels\n",
    "# data['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# # Plotting the clusters with sentiment scores\n",
    "# plt.figure(figsize=(10, 8))\n",
    "\n",
    "# # Plot each cluster separately\n",
    "# for cluster_label in range(2):\n",
    "#     plt.scatter(pca_matrix[data['cluster_label'] == cluster_label, 0], \n",
    "#                 pca_matrix[data['cluster_label'] == cluster_label, 1], \n",
    "#                 c=data[data['cluster_label'] == cluster_label]['compound_score'], \n",
    "#                 cmap='coolwarm', alpha=0.7, label=f'Cluster {cluster_label}')\n",
    "\n",
    "# plt.title('PCA Clustering of Tweets with Sentiment Scores')\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.colorbar(label='Compound Sentiment Score')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# pca = PCA()\n",
    "# pca.fit(bow_matrix.toarray())\n",
    "# plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Explained Variance Ratio')\n",
    "# plt.title('Explained Variance Ratio')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PCA only support sparse inputs with the \"arpack\" solver, while \"auto\" was passed. See TruncatedSVD for a possible alternative.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Step 4: Dimensionality Reduction with PCA\u001b[39;00m\n\u001b[1;32m     58\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m X_pca \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_bow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Plot PCA\u001b[39;00m\n\u001b[1;32m     62\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/COGS118B_WI24/lib/python3.9/site-packages/sklearn/utils/_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 273\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    274\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    275\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    278\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    279\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/COGS118B_WI24/lib/python3.9/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/COGS118B_WI24/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:454\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    432\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    433\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \n\u001b[1;32m    435\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m     U, S, Vt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[1;32m    455\u001b[0m     U \u001b[39m=\u001b[39m U[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components_]\n\u001b[1;32m    457\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwhiten:\n\u001b[1;32m    458\u001b[0m         \u001b[39m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/COGS118B_WI24/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:472\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[39m# Raise an error for sparse input and unsupported svd_solver\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39mif\u001b[39;00m issparse(X) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msvd_solver \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marpack\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 472\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    473\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mPCA only support sparse inputs with the \u001b[39m\u001b[39m\"\u001b[39m\u001b[39marpack\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m solver, while \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    474\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msvd_solver\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m was passed. See TruncatedSVD for a possible\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    475\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m alternative.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    476\u001b[0m     )\n\u001b[1;32m    477\u001b[0m \u001b[39m# Raise an error for non-Numpy input and arpack solver.\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msvd_solver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marpack\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m is_array_api_compliant:\n",
      "\u001b[0;31mTypeError\u001b[0m: PCA only support sparse inputs with the \"arpack\" solver, while \"auto\" was passed. See TruncatedSVD for a possible alternative."
     ]
    }
   ],
   "source": [
    "# # Step 1: Preprocessing the Data\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# data = pd.read_csv('suicide.csv')\n",
    "\n",
    "# def clean_text(text):\n",
    "#     try:\n",
    "#         if pd.isna(text):  # Check if text is NaN\n",
    "#             return ''\n",
    "#         text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "#         text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
    "#         text = re.sub(r'&\\w+;', '', text)    # Remove HTML entities\n",
    "#         text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "#         text = text.lower()                  # Convert to lowercase\n",
    "#         return text\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error cleaning text: {e}, Text: {text}\")\n",
    "#         return ''  # Return empty string if cleaning fails\n",
    "\n",
    "# data['cleaned_text'] = data['Tweet'].apply(clean_text)\n",
    "# data.head()\n",
    "\n",
    "# # Step 2: Feature Extraction using Bag of Words (BoW)\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X_bow = vectorizer.fit_transform(data['cleaned_text'])\n",
    "\n",
    "# # Step 3: Sentiment Analysis using VADER\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# def vader_sentiment(text):\n",
    "#     scores = sid.polarity_scores(text)\n",
    "#     return scores['compound']\n",
    "\n",
    "# data['sentiment_score'] = data['cleaned_text'].apply(vader_sentiment)\n",
    "\n",
    "# # Step 4: Dimensionality Reduction\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# svd = TruncatedSVD(n_components=100)\n",
    "# X_svd = svd.fit_transform(X_bow)\n",
    "\n",
    "# # Step 5: Clustering\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# kmeans = KMeans(n_clusters=2)\n",
    "# kmeans.fit(X_svd)\n",
    "# data['cluster'] = kmeans.labels_\n",
    "\n",
    "# # Step 4: Dimensionality Reduction with PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# X_pca = pca.fit_transform(X_bow)\n",
    "\n",
    "# # Plot PCA\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(X_pca[:, 0], X_pca[:, 1], c=data['cluster'], cmap='viridis', alpha=0.5)\n",
    "# plt.title('PCA Visualization')\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.colorbar(label='Cluster')\n",
    "# plt.show()\n",
    "\n",
    "# # Step 5: Plot KMeans Clustering\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='viridis', alpha=0.5)\n",
    "# plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', marker='X', label='Centroids')\n",
    "# plt.title('KMeans Clustering')\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.colorbar(label='Cluster')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Step 6: Labeling\n",
    "# # You might label clusters based on manual inspection or domain knowledge\n",
    "\n",
    "# # Step 7: Evaluation\n",
    "# # Evaluate the clustering algorithm using metrics like silhouette score or purity\n",
    "\n",
    "# # Step 8: Deployment (Optional)\n",
    "# # Deploy the model or pipeline to predict whether new tweets are potential suicide posts or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('COGS118B_WI24')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d32b38566dea232296ad015676f5eb17bd884ce0b9315fa59364119c2a8275d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
