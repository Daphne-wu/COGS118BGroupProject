{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # import pandas as pd\n",
    "# # # from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# # # from sklearn.decomposition import TruncatedSVD\n",
    "# # # from sklearn.cluster import KMeans\n",
    "# # # from sklearn.model_selection import train_test_split\n",
    "# # # from sklearn.metrics import accuracy_score\n",
    "# # # from sklearn.pipeline import Pipeline\n",
    "# # # from nltk.corpus import stopwords\n",
    "# # # from nltk.stem import PorterStemmer\n",
    "# # # from sklearn.preprocessing import LabelEncoder\n",
    "# # # from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # # # Load data\n",
    "# # # data = pd.read_csv('suicide.csv')\n",
    "\n",
    "# # # # Preprocessing\n",
    "# # # stop_words = set(stopwords.words('english'))\n",
    "# # # stemmer = PorterStemmer()\n",
    "\n",
    "# # # def preprocess_text(text):\n",
    "# # #     text = text.lower()\n",
    "# # #     text = ' '.join([word for word in text.split() if word.isalpha() and word not in stop_words])\n",
    "# # #     text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "# # #     return text\n",
    "\n",
    "# # # data['clean_text'] = data['Tweet'].apply(preprocess_text)\n",
    "\n",
    "# # # # Feature extraction\n",
    "# # # tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "# # # tfidf_matrix = tfidf_vectorizer.fit_transform(data['clean_text'])\n",
    "\n",
    "# # # # Dimensionality reduction\n",
    "# # # svd = TruncatedSVD(n_components=100)\n",
    "# # # svd_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# # # # Clustering\n",
    "# # # kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# # # kmeans.fit(svd_matrix)\n",
    "\n",
    "# # # # Assign cluster labels\n",
    "# # # data['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# # # # Split data into train and test sets\n",
    "# # # X_train, X_test, y_train, y_test = train_test_split(svd_matrix, data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# # # # Train classifier\n",
    "# # # classifier = LogisticRegression()\n",
    "# # # classifier.fit(X_train, y_train)\n",
    "\n",
    "# # # # Predictions\n",
    "# # # y_pred = classifier.predict(X_test)\n",
    "\n",
    "# # # # Evaluate classifier\n",
    "# # # accuracy = accuracy_score(y_test, y_pred)\n",
    "# # # print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# # import pandas as pd\n",
    "# # from sklearn.feature_extraction.text import CountVectorizer\n",
    "# # from sklearn.decomposition import TruncatedSVD\n",
    "# # from sklearn.cluster import KMeans\n",
    "# # from sklearn.model_selection import train_test_split\n",
    "# # from sklearn.metrics import accuracy_score\n",
    "# # from sklearn.pipeline import Pipeline\n",
    "# # from nltk.corpus import stopwords\n",
    "# # from nltk.stem import PorterStemmer\n",
    "# # from sklearn.preprocessing import LabelEncoder\n",
    "# # from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # # Load data\n",
    "# # data = pd.read_csv('suicide.csv')\n",
    "\n",
    "# # # Preprocessing\n",
    "# # stop_words = set(stopwords.words('english'))\n",
    "# # stemmer = PorterStemmer()\n",
    "\n",
    "# # def preprocess_text(text):\n",
    "# #     text = text.lower()\n",
    "# #     text = ' '.join([word for word in text.split() if word.isalpha() and word not in stop_words])\n",
    "# #     text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "# #     return text\n",
    "\n",
    "# # data['clean_text'] = data['Tweet'].apply(preprocess_text)\n",
    "\n",
    "# # # Feature extraction (Bag of Words)\n",
    "# # count_vectorizer = CountVectorizer(max_features=1000)\n",
    "# # bow_matrix = count_vectorizer.fit_transform(data['clean_text'])\n",
    "\n",
    "# # # Dimensionality reduction\n",
    "# # svd = TruncatedSVD(n_components=100)\n",
    "# # svd_matrix = svd.fit_transform(bow_matrix)\n",
    "\n",
    "# # # Clustering\n",
    "# # kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# # kmeans.fit(svd_matrix)\n",
    "\n",
    "# # # Assign cluster labels\n",
    "# # data['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# # # Split data into train and test sets\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(svd_matrix, data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# # # Train classifier\n",
    "# # classifier = LogisticRegression()\n",
    "# # classifier.fit(X_train, y_train)\n",
    "\n",
    "# # # Predictions\n",
    "# # y_pred = classifier.predict(X_test)\n",
    "\n",
    "# # # Evaluate classifier\n",
    "# # accuracy = accuracy_score(y_test, y_pred)\n",
    "# # print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# # Load data\n",
    "# data = pd.read_csv('suicide.csv')\n",
    "\n",
    "# # Drop rows with missing values in the 'Tweet' column\n",
    "# data.dropna(subset=['Tweet'], inplace=True)\n",
    "\n",
    "# # Feature extraction (Bag of Words)\n",
    "# count_vectorizer = CountVectorizer(max_features=1000)\n",
    "# bow_matrix = count_vectorizer.fit_transform(data['Tweet'])\n",
    "\n",
    "# # Dimensionality reduction\n",
    "# svd = TruncatedSVD(n_components=100)\n",
    "# svd_matrix = svd.fit_transform(bow_matrix)\n",
    "\n",
    "# # Clustering\n",
    "# kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# kmeans.fit(svd_matrix)\n",
    "\n",
    "# # Assign cluster labels\n",
    "# data['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# # Evaluate clustering results (compare with Suicide labels)\n",
    "# # Implement your evaluation steps here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# # Load data\n",
    "# data = pd.read_csv('suicide.csv')\n",
    "\n",
    "# # Drop rows with missing values in the 'Tweet' column\n",
    "# data.dropna(subset=['Tweet'], inplace=True)\n",
    "\n",
    "# # Feature extraction (Bag of Words)\n",
    "# count_vectorizer = CountVectorizer(max_features=1000)\n",
    "# bow_matrix = count_vectorizer.fit_transform(data['Tweet'])\n",
    "\n",
    "# # Dimensionality reduction with PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_matrix = pca.fit_transform(bow_matrix.toarray())\n",
    "\n",
    "# # Clustering\n",
    "# kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# kmeans.fit(pca_matrix)\n",
    "\n",
    "\n",
    "# # Assign cluster labels\n",
    "# data['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# # Plotting the clusters\n",
    "# plt.figure(figsize=(8, 6))\n",
    "\n",
    "# # Plot each cluster separately\n",
    "# for cluster_label in range(2):\n",
    "#     plt.scatter(pca_matrix[data['cluster_label'] == cluster_label, 0], \n",
    "#                 pca_matrix[data['cluster_label'] == cluster_label, 1], \n",
    "#                 label=f'Cluster {cluster_label}', alpha=0.5)\n",
    "\n",
    "# # Plot centroids\n",
    "# centroids = kmeans.cluster_centers_\n",
    "# plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=100, c='black', label='Centroids')\n",
    "# # \n",
    "# plt.title('K-Means Clustering of Tweets')\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Visualize clustering results\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(pca_matrix[:, 0], pca_matrix[:, 1], c=data['cluster_label'], cmap='viridis', alpha=0.5)\n",
    "# plt.title('PCA Clustering of Tweets')\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.colorbar(label='Cluster Label')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.cluster import KMeans\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# # Load data\n",
    "# data = pd.read_csv('suicide.csv')\n",
    "\n",
    "# # Drop rows with missing values in the 'Tweet' column\n",
    "# data.dropna(subset=['Tweet'], inplace=True)\n",
    "\n",
    "# # Initialize VADER sentiment analyzer\n",
    "# sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# # Calculate sentiment scores for each tweet\n",
    "# data['compound_score'] = data['Tweet'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "\n",
    "# # Feature extraction (Bag of Words)\n",
    "# count_vectorizer = CountVectorizer()\n",
    "# bow_matrix = count_vectorizer.fit_transform(data['Tweet'])\n",
    "\n",
    "# # Dimensionality reduction with PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_matrix = pca.fit_transform(bow_matrix.toarray())\n",
    "\n",
    "# # Clustering with K-means\n",
    "# kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# kmeans.fit(pca_matrix)\n",
    "\n",
    "# # Assign cluster labels\n",
    "# data['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# # Plotting the clusters with sentiment scores\n",
    "# plt.figure(figsize=(10, 8))\n",
    "\n",
    "# # Plot each cluster separately\n",
    "# for cluster_label in range(2):\n",
    "#     plt.scatter(pca_matrix[data['cluster_label'] == cluster_label, 0], \n",
    "#                 pca_matrix[data['cluster_label'] == cluster_label, 1], \n",
    "#                 c=data[data['cluster_label'] == cluster_label]['compound_score'], \n",
    "#                 cmap='coolwarm', alpha=0.7, label=f'Cluster {cluster_label}')\n",
    "\n",
    "# plt.title('PCA Clustering of Tweets with Sentiment Scores')\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.colorbar(label='Compound Sentiment Score')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# pca = PCA()\n",
    "# pca.fit(bow_matrix.toarray())\n",
    "# plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Explained Variance Ratio')\n",
    "# plt.title('Explained Variance Ratio')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Preprocessing the Data\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# data = pd.read_csv('suicide.csv')\n",
    "\n",
    "# def clean_text(text):\n",
    "#     try:\n",
    "#         if pd.isna(text):  # Check if text is NaN\n",
    "#             return ''\n",
    "#         text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "#         text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
    "#         text = re.sub(r'&\\w+;', '', text)    # Remove HTML entities\n",
    "#         text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "#         text = text.lower()                  # Convert to lowercase\n",
    "#         return text\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error cleaning text: {e}, Text: {text}\")\n",
    "#         return ''  # Return empty string if cleaning fails\n",
    "\n",
    "# data['cleaned_text'] = data['Tweet'].apply(clean_text)\n",
    "# data.head()\n",
    "\n",
    "# # Step 2: Feature Extraction using Bag of Words (BoW)\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X_bow = vectorizer.fit_transform(data['cleaned_text'])\n",
    "\n",
    "# # Step 3: Sentiment Analysis using VADER\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# def vader_sentiment(text):\n",
    "#     scores = sid.polarity_scores(text)\n",
    "#     return scores['compound']\n",
    "\n",
    "# data['sentiment_score'] = data['cleaned_text'].apply(vader_sentiment)\n",
    "\n",
    "# # Step 4: Dimensionality Reduction\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# svd = TruncatedSVD(n_components=100)\n",
    "# X_svd = svd.fit_transform(X_bow)\n",
    "\n",
    "# # Step 5: Clustering\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# kmeans = KMeans(n_clusters=2)\n",
    "# kmeans.fit(X_svd)\n",
    "# data['cluster'] = kmeans.labels_\n",
    "\n",
    "# # Step 4: Dimensionality Reduction with PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# X_pca = pca.fit_transform(X_bow)\n",
    "\n",
    "# # Plot PCA\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(X_pca[:, 0], X_pca[:, 1], c=data['cluster'], cmap='viridis', alpha=0.5)\n",
    "# plt.title('PCA Visualization')\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.colorbar(label='Cluster')\n",
    "# plt.show()\n",
    "\n",
    "# # Step 5: Plot KMeans Clustering\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='viridis', alpha=0.5)\n",
    "# plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', marker='X', label='Centroids')\n",
    "# plt.title('KMeans Clustering')\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.colorbar(label='Cluster')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Step 6: Labeling\n",
    "# # You might label clusters based on manual inspection or domain knowledge\n",
    "\n",
    "# # Step 7: Evaluation\n",
    "# # Evaluate the clustering algorithm using metrics like silhouette score or purity\n",
    "\n",
    "# # Step 8: Deployment (Optional)\n",
    "# # Deploy the model or pipeline to predict whether new tweets are potential suicide posts or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('COGS118B_WI24')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d32b38566dea232296ad015676f5eb17bd884ce0b9315fa59364119c2a8275d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
